# Transformers and finetuning with LLMs
### Part A
a) Implement nanogpt from scratch using your own code in pytorch. Take help from code interpreter gpt4 plugin. The code should be modular in colab and should have same caliber of debugging as the original colab 
### Medium Article
[Implementing NanoGPT from Scratch Using PyTorch: A Guide with “Pride and Prejudice”](https://medium.com/@kelly.nguyen01/implementing-nanogpt-from-scratch-using-pytorch-a-guide-with-pride-and-prejudice-58f3b51f9a84) 
<img width="1265" alt="Screenshot 2024-12-01 at 4 55 07 PM" src="https://github.com/user-attachments/assets/f6fc656c-ce61-49c2-a427-aae6afea4ed7"><img width="1077" alt="Screenshot 2024-12-01 at 4 54 46 PM" src="https://github.com/user-attachments/assets/3cca4233-2110-4d95-bf28-33b3c481261c">

### Part B
b) Implement "textbooks are all you need" case study with your own data
